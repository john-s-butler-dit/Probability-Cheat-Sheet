\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\def\Pr{\mathop{\rm Pr}}
\usepackage[landscape,margin=1cm]{geometry}
\usepackage[english]{babel}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\usepackage{multicol}
% colour themes to come. KnitR?

%-------------------------

\title{Probability and Statistical Inference}
\author{John S Butler }
%\date{July 2019}
\input{cheatsheet-template.tex}



%--------------------------------------------------------------------------------
\begin{document}
\small
\begin{multicols}{3}

%\maketitle
%\thispagestyle{empty}
\scriptsize
%\tableofcontents


%\section{Data Type}
\section*{Probability and Statistical Inference}
\subsection*{Cheat Sheet}
%\href{https://sites.google.com/dit.ie/math4001
%}{Course} Cheat Sheet}

\subsubsection*{\href{johnsbutler.netlify.com}{John S Butler} (TU Dublin) }
\subsubsection*{\href{https://twitter.com/StatisticalInf1}{Course Twitter Account} }
\begin{textbox}{Data Type}
\begin{multicols}{2}
\begin{itemize}
  \item Categorical
  \item Interval
  \item Ordinal
  \item Ratio
\end{itemize}

\end{multicols}

\end{textbox}


\begin{textbox}{Measures of Location}
Different aspects of a distribution of data can be summarised by the measures of location:
\begin{enumerate}
    \item The First Moment: Mean, Mode or Median;
    \item  The Second Moment: Variance, Standard Deviation;
    \item  The Third Moment: Skewness.
\end{enumerate}

\begin{subbox}{subbox}{First Moment: Middle}
\begin{center}
\tiny
    \includegraphics[width=\textwidth]{Figures/Measures_of_Location/middle.png}
    Wiggles Concert Attendance
\end{center}

\end{subbox}

\begin{subbox}{subbox}{Second Moment: Spread}
\begin{center}
\tiny
    \includegraphics[width=\textwidth]{Figures/Measures_of_Location/range.png}
\end{center}

\end{subbox}
\end{textbox}
%% NEW TEXT BOX MEASURES OF LOCATION
\begin{textbox}{Measures of Location (cont.)}

\begin{subbox}{subbox}{Third Moment: Symmetry}
\begin{center}
\tiny
    \includegraphics[width=\textwidth]{Figures/Measures_of_Location/skew.png}
Left: One Direction Concert Attendance \\ Middle: Harry Styles Concert Attendance \\ Right: Andr√© Rieu Concert Attendance
\end{center}
\end{subbox}
\end{textbox}
\begin{textbox}{Mathematical Probability }
\begin{subbox}{subbox}{Definitions}
\tiny
Define some event $A$ that can be the outcome of an experiment.\\
$\Pr(A)$ is the probability of a given event $A$ will happen.\\
Rules:
\begin{itemize}
    \item $\Pr(A)$ is between $0$ and $1$, $0\leq Pr(A) \leq 1$;
    \item $\Pr(A)=1$, means it will definitely happen;
    \item $\Pr(A)=0$, means it will definitely \textbf{not} happen;
    \item $\Pr(A)=0.05$, is arbitrarily considered unlikely.
\end{itemize}
\end{subbox}

\begin{subbox}{subbox}{Sample Space and Events}
\tiny
The \textbf{Sample Space}, $S$, of an experiment is the universal set of all possible outcomes for that experiment, defined so, no two outcomes can occur simultaneously. For example:
\begin{itemize}
    \item Throwing a die $S=\{1,2,3,4,5,6\};$
    \item Tossing two coins $S=\{HH,TH,HT,TT\}.$
\end{itemize}

An event, $A$, is a subset of the sample space $S$. For example:
\begin{itemize}
    \item Throwing a die $S=\{3,4,6\};$
    \item Tossing two coins $S=\{TH,TT\}.$
\end{itemize}
\end{subbox}

\begin{subbox}{subbox}{Axioms of Probabilities}
\tiny
For an event $A$ subset $S$ associated a number $Pr(A)$, the probability of $A$, which must have the following properties

\begin{itemize}
     \item $\Pr(A \bigcap B)=0;$ $\Pr(A\bigcup B)= \Pr(A)+\Pr(B) $;
     \item Probability of the Null Event $\Pr(\emptyset)=0$;
     \item The probability of the complement of $A,$ $\Pr(\bar{A})=1-\Pr(A)$;
     \item $\Pr(A \bigcup B)= \Pr(A)+\Pr(B)-\Pr(A\bigcap B)$.
\end{itemize}
\end{subbox}


\end{textbox}


\begin{textbox}{Conditional Probability}
 The Conditional Probability $\Pr(A|B)$ denotes the probability of the event $A$ occurring given that the event $B$ has occurred,
\[ \Pr(A|B)=\frac{\Pr(A\bigcap B)}{\Pr(B)}.\]
\begin{subbox}{subbox}{Example: The rain in Ireland}
\tiny
A normal probability would be what is the probability it is going to rain, $\Pr(\text{rain})$.\\
A conditional probability would, be what is the probability it is going to rain \textbf{given} that you are in Ireland, $\Pr(\text{rain}|\text{Ireland}),$
\[ \Pr(\text{rain}|\text{Ireland})=\frac{\Pr(\text{rain}\bigcap \text{Ireland})}{\Pr(\text{Ireland})},\]
where the probability of rain is $\Pr(\text{rain})=0.3$, the probability of being in Ireland is $\Pr(\text{Ireland})=0.4$ and the probability of being in Ireland and it raining is $\Pr(\text{rain}\bigcap \text{Ireland})=0.2$,
\[ \Pr(\text{rain}|\text{Ireland})=\frac{0.2}{0.4}=0.5,\]
You could be interested in the probability that you are in Ireland \textbf{given} that it is raining, 
\[ \Pr(\text{Ireland}|\text{rain})=\frac{\Pr(\text{rain}\bigcap \text{Ireland})}{\Pr(\text{rain})}=\frac{0.2}{0.3}=0.75.\]
\end{subbox}
\end{textbox}
\begin{textbox}{Bayes Theorem}
 Bayes Theorem states  
\[ \Pr(A|B)=\frac{\Pr(B|A)P(A)}{\Pr(B)}.\]
\begin{subbox}{subbox}{Example: Diagnostic test}
\tiny
The probability that an individual has a rare disease is $\Pr(\text{Disease})=0.01$. The probability that a diagnostic test results in a positive (+) test \textit{given you have} the disease is $\Pr(+|\text{Disease})=0.95$. On the other hand, the probability that the diagnostic test results in a positive (+) test \textit{given you do not have} the disease is $\Pr(+|\text{No Disease})=0.1$. 
This raises the important question if you are given a positive diagnosis, what is the probability you have the disease $\Pr(\text{Disease}|+)$? 
From Bayes Theorem we have:
\[ \Pr(\text{Disease}|+)=\frac{\Pr(+|\text{Disease})\Pr(\text{Disease})}{\Pr(+)}\]
The probability of a positive test is,
\[\Pr(+)= \Pr(+|\text{Disease})\Pr(\text{Disease})+\Pr(+|\text{No Disease})\Pr(\text{No Disease}),\]
\[\Pr(+)= 0.1085.\]
\[ \Pr(\text{Disease}|+)=\frac{\Pr(+|\text{Disease})\Pr(\text{Disease})}{\Pr(+)}=\frac{0.95\times 0.01}{0.1085}=0.0875576.\]
This can also be done in a simple table format, by assume a population of 10,000 
\begin{center}
 \begin{tabular}{||c |c c |c||} 
 \hline
  Group & + Diagnosis & - Diagnosis & Total \\
 \hline
 Disease & 95 & 5 & 100 \\ 
 \hline
 No Disease & 990 & 8,910 & 9,900 \\
 \hline
 Total & 1,085 & 8,915 & 10,000 \\
 \hline
\end{tabular}
\end{center}
From the table we can calculate the same answer,\\
 \Pr(\text{Disease}|+)=\frac{95}{1085}=0.0875576.\end{subbox}
\end{textbox}
%%% DISCRETE DISTRIBUTION
\begin{textbox}{Discrete Distribution}
\begin{subbox}{subbox}{Probability Mass Functions}
\tiny
\[  \begin{tabular}{r|r|r|r|r|r}
			Event Number $i$&0&1&2&3&4\\
			\hline
			Event Value $x_i$&-1&0&1&2&3\\
			\hline
			Probability of Event $\Pr(x_i)$&0.3&0.1&0.3&0.1&0.2\\
			\end{tabular}
			\]
The expected value of the distribution is:
   	\[\mu=E[X]=\Sigma_{i} x_i \Pr(x_i),\]
\[\Sigma_{i} x_i p(x_i)=-1\times 0.4+0\times 0.1+1\times 0.3+0.1\times 2+0.2\times 3=0.7,\]
The variance of the distribution is:
	\[Var[X]=\Sigma_{i} (x_i-\mu)^2 p(x_i)=\Sigma_{i} (x_i-0.7)^2 p(x_i).\]


\end{subbox}

\begin{subbox}{subbox}{Binomial Distribution}
\tiny
The formula for the Binomial distribution is:
\[\Pr(k)= \left( \begin{array}{c}
n\\
k\\
	\end{array} \right)p^kq^{n-k}, \ \ k=0,1,2,...n, \]
	\[E[k]=np,\ \ \ Var[k]=npq,\]
 where $n$ is the total of games, k is the number of "wins", $p$ is the probability of a "win", $q=1-p$ probability of a "loss". 
 
    \includegraphics[width=\textwidth]{Figures/Distributions/Binomial.png}
\end{subbox}

\begin{subbox}{subbox}{Geometric Distribution}
\tiny
The formula for the Geometric distribution is:
\[\Pr(k)=q^{(k-1)}p, \ \ k=1,2,... \]
	\[E[k]=\frac{1}{p}, \ \ \ Var[k]=\frac{q}{p^2},\]
 $k$ is the number of events until one "win", $p$ is the probability of a "win", $q=1-p$ probability of a "loss". 
    \includegraphics[width=\textwidth]{Figures/Distributions/Geometric.png}
\end{subbox}
\end{textbox}
%%% DISCRETE DISTRIBUTION
\begin{textbox}{Discrete Distribution}

\begin{subbox}{subbox}{Poisson Distribution}
	\tiny
	The formula for the Poisson distribution is:
	\[\Pr(k)=\frac{\lambda^ke^{-\lambda}}{k!}, \ \ k=0,1,2,... \]
	\[E[k]=\lambda, \ \ \ Var[k]=\lambda. \]
where $\lambda$ is the mean and standard deviation of the distribution and k is the number of "wins" in a specified time or space. 

    \includegraphics[width=\textwidth]{Figures/Distributions/Poisson.png}
\end{subbox}

\end{textbox}
\begin{textbox}{Continuous Distribution}
\begin{subbox}{subbox}{Normal Distribution}
    \includegraphics[width=\textwidth]{Figures/Distributions/Normal.png}
\end{subbox}
\begin{subbox}{subbox}{Confidence Intervals}
\begin{center}
\tiny
    \includegraphics[width=\textwidth]{Figures/Distributions/CI_Normal.png}
\end{center}
\end{subbox}

\end{textbox}
\begin{textbox}{Hypothesis Testing}
Five steps for Hypothesis testings
\begin{enumerate}
\item  State the Null Hypothesis $H_0$;
\item  State an Alternative Hypothesis $H_{\alpha}$;
\item  Calculate a Test Statistic (see below);
\item  Calculate a p-value and/or set a rejection region;
\item  State your conclusions.
\end{enumerate}
\end{textbox}
\begin{textbox}{z-test}
\begin{subbox}{subbox}{Continuous Data}
The test statistic is given by
\[Z=\frac{\bar{x}-\mu}{\frac{\sigma}{\sqrt{n}}} \sim \mathcal{N}(0,1), \]
where $\bar{x}$ is the observed mean, $\mu$ is the historical mean, $\sigma$ is the standard deviation and $n$ is the number of observations.
$\mathcal{N}(0,1)$ is the normal distribution with a mean of 0 and a standard deviation of 1.
\begin{subbox}{subbox}{Do supplements make you faster? }
\tiny
The effect of a food supplements on the response time in rats is of interest to a biologist. They have established that the normal response time of rats is $\mu=1.2$ seconds. The $n=100$ rats were given a new food supplements. The following summary statistics were recorded from the data $\bar{x}=1.05$ and $\sigma= 0.5$ seconds
\begin{enumerate}
    \item The rats in the study are the same as normal rats,  $H_0 : \bar{x}=1.2$. 
\item The rats are different, $H_\alpha:\bar{x}\not= 1.2$.
\item Calculate a Test Statistic $Z=\frac{1.05-1.2}{\frac{0.5}{\sqrt{100}}}=-3 $
\item Reject the Null hypothesis $H_0$ if $Z<-1.96$ and $Z>1.96$
\item The data suggests that rats are faster with the new food.
\end{enumerate}


\end{subbox}
\end{subbox}
\begin{subbox}{subbox}{Proportional Data}
The test statistic is given by
\[z=\frac{\hat{p}-p}{\sqrt{\frac{pq}{n}}} \sim \mathcal{N}(0,1). \]
where $\hat{p}$ is the observed proportion, $p$ is the historical proportion, $q$ is the complement $q=1-p$,  and $n$ is the number of observations.

\end{subbox}
\end{textbox}
\begin{textbox}{t-test}
\begin{subbox}{subbox}{paired t-test}

 The test statistic is given by  \[ t={\frac {{\bar {x}}-{\bar \mu_0}}{\frac{s}{\sqrt n}}} \sim t_{\alpha,df}\]
where $\bar{x}$ is the observed mean, $\mu_0$ is the null mean, $s$ is the standard deviation and $n$ is the number of observations.
$\alpha$ is the alpha level and df is the degrees of freedom.
\end{subbox}
\begin{subbox}{subbox}{unpaired t-test}
The test statistic is given by
\[ t={\frac {{\bar {x}}_{1}-{\bar {x}}_{2}}{s_{p}{\sqrt {\frac {1}{n_1}+\frac{1}{n_2}}}}} \sim t_{\alpha,df}\]
 where
$s_{p}={\sqrt{\frac {s_{x_{1}}^{2}+s_{x_{2}}^{2}}{2}}}$ is the pooled sample standard deviation, $\bar{x}_1$ and $\bar{x}_2$ are the sample means, $n_1$ and $n_2$ are the sample sizes.
\end{subbox}
\end{textbox}
\begin{textbox}{$\chi^2$ Independence test}
The test statistic to test if data are independent of group is given by:
   	\[\chi^2_{Ind}=\sum \frac{(O-E)^2}{E} \sim \chi^2_{(r-1)(c-1)}.\]
where $O$ is the observed data, $E$ is the expected data if independent, $r$ is the number of rows and $c$ is the number of columns.\\
\begin{subbox}{subbox}{Does ice-cream flavour matter?}
\tiny
An ice-cream company had 500 people sample one of three different ice-cream flavours and asked them to say whether they liked or disliked the ice-cream. 
\begin{center}
\begin{tabular}{|l|r|r|r|}
\hline
&Vanilla&Chocolate& Strawberry\\
\hline
Liked&130&170&100\\
\hline
Disliked&20&30&50\\
\hline
\end{tabular}
\end{center}
The $\chi^2_{Ind}$ independence test could be used to determine if the enjoyment of the ice-cream depends on the flavour.\\
\end{subbox}
\end{textbox}
\begin{textbox}{$\chi^2$ Goodness of Fit}
The test statistic to test if data come from a specific distribution is given by:
\[\chi^2_{GoF}=\sum \frac{(O-E)^2}{E} \sim\chi^2_{k-1},\]
where $O$ is the observed data, $E$ is the expected data from a chosen distribution and $k$ is the number of observation bins.
\begin{subbox}{subbox}{Does it fit?}
\tiny
The $\chi^2_{GoF}$ can test if the observed distribution of the height of Dutch people (grey) fits the expected distribution of heights (dark grey).

\includegraphics[width=\textwidth]{Figures/Distributions/GoF.png}
\end{subbox}
\end{textbox}

\begin{textbox}{Linear Regression}

A linear regression is used to model a linear relationship of the dependent variable $y$ and the regressors $x_1$, $x_2$, ...
	\[  y=\beta_0+\beta_1 x_{1} +\beta_2 x_{2}+..., \]
	where $\beta_0$, $\beta_1$ are the slopes of the regressors.

\begin{subbox}{subbox}{Height Prediction}
\tiny
A simple linear regression (correlation) is used to predict the height of 744 children $y$ using the height of their parent $x$, 
\[  y=\beta_0+\beta_1 x.\]
The plot below shows the fit of the model:
\begin{center}
\includegraphics[width=\textwidth]{Figures/Regression/Linear_Regression.png}
\end{center}
The parents' height $x$ explained $12.7\%$ of the childrens' height $y$.
\end{subbox}
\end{textbox}
\begin{textbox}{Logistic Regression}
A logistic regression (or logit model) is used to model the probability of a binary events such as win/lose.
The general formula for the Logistic regression is 
\[  p_i=\frac{e^{\eta}}{1+e^{\eta}},\]
where
\[\eta=\beta_0+\beta_1 x_{1} +\beta_2 x_{2}+... \]
and $\beta$ is the slope corresponding to the predictor variable $x$.
\begin{subbox}{subbox}{Sexton Conversion Rate}
\tiny
Data from $1000$ conversions kicks by Johnny Sexton was acquired; the  distance (m) from the goal-line and if the kick was a  miss $0$  or a conversion $1$. The data  was fit to a logistic regression. The model was  
\[  p=\frac{e^{\eta}}{1+e^{\eta}},\]
where
\[\eta=\beta_0+\beta_1 \text{Distance} \]
and $p$ is the probability of a conversion.
The plot below shows the fit of the model:
\begin{center}
\includegraphics[width=\textwidth]{Figures/Regression/Sexton.png}
\end{center}
The model predicts that at the half-way line (50m) Sexton has a  $0.375$ probability of conversion.

\end{subbox}
\end{textbox}
\begin{textbox}{Bibliography}
\begin{enumerate}
\item 
Devore \& Peck  - Statistics: The exploration and analysis of data (2011)
\item James, G., Witten, D., Hastie, T., \& Tibshirani, R. (2013). An introduction to statistical learning (Vol. 112, p. 18). New York: springer 
\href{https://www.statlearning.com}{book website}.
\item Poldrack R. Statistical Thinking in the 21st Century 2020 \href{https://statsthinking21.github.io/statsthinking21-core-site/index.html}{website}.
\item
Gareth, J., et al. - An introduction to statistical learning. Vol. 112. New York: Springer, 2013.
\item
Fry, H. - Hello World: How to be Human in the Age of the Machine, Doubleday, 2018
\item Alexander,  R. - Telling Stories with Data 2022 \href{https://tellingstorieswithdata.com}{website}
\item Butler, J. S., \href{https://github.com/john-s-butler-dit/Probability_and_Statistical_Inference}{Course GitHub Repository}  

\end{enumerate}


\end{textbox}
\begin{textbox}{Notation}
\begin{itemize}
\item 
$\bar{x}$- mean of a list of numbers $x_i$
\item 
$\sigma$ - standard deviation of a list of numbers $x_i$
\item 
$\sigma^2$ - variance of a list of numbers 
\item $\Pr{(A)}$ - probability of event $A$
\item $\Pr{(\bar{A})}$ - probability of not event $A$
\item $\Pr{(A|B)}$ - probability of event $A$ given event $B$ is known
\item $\Sigma_{i}^{n}x_i$ - the sum of a list of number $x_i$
\item $n!$ - $n$ factorial is $n\times(n-1)\times\cdots \times 1$
\item $5!$ - $5$ factorial is $5\times(5-1)\times(5-2)\times(5-3)\times(5-4)=5\times4\times3\times2\times 1=120$
\item $\binom{n}{k}=^nC_k $ - $n$ choose $k$ equals to $\frac{n!}{k!(n-k)!}$
\item $\binom{5}{3}=^5C_3 $ - $5$ choose $3$ equals to $\frac{5!}{3!(5-3)!}=\frac{5!}{3!2!}=\frac{5\times 4\times 3 \times 2 \times 1}{3\times 2 \times 1\times 2 \times 1 }=10$
\item $^nP_k $ - $n$ pick $k$ equals to $\frac{n!}{(n-k)!}$
\item $^5P_3 $ - $5$ pick $3$ equals to $\frac{5!}{(5-3)!}=\frac{5!}{2!}=\frac{5\times 4\times 3 \times 2 \times 1}{ 2 \times 1 }=60$
\item $p$ - $p$ probability of a "win"
\item $q$ - $q$ probability of a "loss" $1-p$
\item $p^n$ - $p$ to the power of $n$ is $p\times p\times \cdots \times p$
\item $0.1^4$ - $0.1$ to the power of $4$ is $0.1\times 0.1\times 0.1\times 0.1 \times 0.1$
\item $E[X]$ - the expected value of a probability distribution
\item $Var[X]$ - the variance of a probability distribution
\item $e$ - is the exponential which is it equal to approximately $2.718$ it is comes up again and again in mathematics formulas
\item $H_0$ - null hypothesis 
\item $H_\alpha$ - alternative hypothesis 
\item $\mu$ - real mean (generally never known)
\item $\bar{x}$ - observed mean given the data
\item $\hat {p}$ - is the observed sample proportion
\item $\hat {p}$ - is the observed sample proportion
\item $\mathcal{N}(\mu,\sigma)$ - is the Guassian distribution with mean $\mu$ and standard deviation $\sigma$
\item $\mathcal{N}(0,1)$ - is a special case of Guassian distribution known as the Normal Distribution with mean $0$ and standard deviation $1$
\item df-degrees of freedom
\item $\chi^2_{df}$ - Chi ($\chi$)-squared ($^2$) distribution with degrees of freedom df

\item $\beta$ the coefficient for a regression
\item $\hat{\beta}$ the coefficient estimated for a regression from the observed 

\end{itemize}


\end{textbox}
\end{multicols}

\end{document}
